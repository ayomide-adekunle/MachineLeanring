{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "evoJuaWZdLKf"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jnte6x_dLKi"
      },
      "source": [
        "## CompositeLayer is an nn.Module with the following layers (in order):\n",
        "\n",
        "### 1. BatchNorm2d with default parameters\n",
        "### 2. ReLU\n",
        "### 3. Conv2d with kernel_size=1, stride=1, bias=False, all others default values\n",
        "### 4. BatchNorm2d with default parameters\n",
        "### 5. ReLU\n",
        "### 6. Conv2d with kernel_size=3, stride=1, padding=1, bias=False, all others default values\n",
        "\n",
        "#### These layers are connected in a sequential order (i.e., 1-2-3-4-5-6)\n",
        "\n",
        "## Fill the blanks marked by \"##ToAdd:\" in the following cell (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzzZdc0xdLKj"
      },
      "source": [
        "class CompositeLayer(nn.Module):\n",
        "    def __init__(self, num_input_features, num_hidden_features, num_output_features):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            num_input_features: int, num_features for the first BatchNorm2d\n",
        "            num_hidden_features: int, out_channels for the first Conv2d\n",
        "            num_output_features: int, out_channels for the second Conv2d\n",
        "        \"\"\"\n",
        "        super(CompositeLayer, self).__init__()\n",
        "        ##ToAdd: layer BatchNorm2d with default parameters (5 points)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(num_input_features)\n",
        "        \n",
        "        ##EndAdd\n",
        "        self.relu1 = nn.ReLU(inplace=True) \n",
        "        ##ToAdd: layer Conv2d with kernel_size=1, stride=1, bias=False, all others default values (5 points)\n",
        "        self.conv1 = nn.Conv2d(num_input_features, num_hidden_features, \n",
        "                               kernel_size=1, stride=1, bias=False)\n",
        "        \n",
        "        ##EndAdd\n",
        "        ##ToAdd: layer BatchNorm2d with default parameters (5 points)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(num_hidden_features)\n",
        "        \n",
        "        ##EndAdd\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        ##ToAdd: layer Conv2d with kernel_size=3, stride=1, padding=1, bias=False, all others default values (5 points)\n",
        "        self.conv2 = nn.Conv2d(num_hidden_features, num_output_features, kernel_size=3, stride=1, padding=1,bias=False)\n",
        "        ##EndAdd\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: a torch.Tensor or a list of torch.Tensor\n",
        "        \"\"\"\n",
        "        if isinstance(inputs, list):\n",
        "            inputs = torch.cat(inputs, 1)\n",
        "        ##ToAdd: forward pass from inputs to outputs (10 points)\n",
        "        x1 = self.batch_norm1(inputs)\n",
        "        r1 = self.relu1(x1)\n",
        "        x2 = self.conv1(r1)\n",
        "        x3 = self.batch_norm2(x2)\n",
        "        r2 = self.relu2(x3)\n",
        "        outputs = self.conv2(r2)\n",
        "        \n",
        "#         outputs = self.conv3(self.relu2(self.norm2(self.conv1(self.relu1(self.norm1(inputs))))))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##EndAdd\n",
        "        return outputs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIdWjSaadLKk"
      },
      "source": [
        "## Block consists of a number (argument `num_layers`) of CompositeLayer defined above:\n",
        "### To illustrate how it works, let's set `num_layers=4` for a Block model. \n",
        "### Then it will have 3 CompositeLayer. Let's name them as `layer1`, `layer2`, `layer3`, and `layer4`.\n",
        "### The input of `layer1` is provided as `init_features` in the `forward` function defined below.\n",
        "### The output of `layer1` will be the input of `layer2`\n",
        "## The output of `layer1` and the output of `layer2` concatenated together at the channel dimension (dim=1) will be the input of `layer3`.\n",
        "## The outputs of `layer1`, `layer2`, and `layer3` concatenated together at the channel dimension (dim=1) will be the input of `layer4`.\n",
        "## So the input of `layer{i}` is the concatenated output of all its previous layers from `layer1` until `layer{i-1}`.\n",
        "## The model `forward` function returns the `init_features` and ALL OUTPUTS of all layers concatenated together at the channel dimension (dim=1)\n",
        "\n",
        "\n",
        "## Fill the blanks marked by \"##ToAdd:\" in the following cell (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGO3GwZtdLKl"
      },
      "source": [
        "class Block(nn.ModuleDict):\n",
        "    def __init__(self, num_layers, num_input_features, num_hidden_features, num_output_features):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            num_layers: int, how many CompositeLayer are included\n",
        "            num_input_features: int, num_input_features for the FIRST CompositeLayer \n",
        "            (YOU NEED TO CALCULATE the num_input_features for all other CompositeLayer)\n",
        "            num_hidden_features: int, num_hidden_features for every CompositeLayer\n",
        "            num_output_features: int, num_output_features for every CompositeLayer\n",
        "        \"\"\"\n",
        "        super(Block, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            ##ToAdd: Add a number of CompositeLayer. Hint: define a CompositeLayer with proper arguments, \n",
        "            ## and use add_module function to add it to self (10 points)\n",
        "            comp_layer = CompositeLayer(num_input_features+(i*num_output_features), num_hidden_features, num_output_features)\n",
        "            self.add_module('comp_layer%d' % (i+1), comp_layer)\n",
        "\n",
        "            ##EndAdd\n",
        "\n",
        "    def forward(self, init_features):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            init_features: torch.Tensor, the input of the first CompositeLayer\n",
        "        \"\"\"\n",
        "        features = [init_features]\n",
        "        ##ToAdd: forward pass. Hint: Append the output of each layer to features one at a time \n",
        "        ##until the last layer (10 points)\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##EndAdd\n",
        "        return torch.cat(features, 1)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVDBh5dsdLKl"
      },
      "source": [
        "## ConvPool is nn.Sequential with the following layers (in order):\n",
        "\n",
        "### 1. BatchNorm2d with default parameters\n",
        "### 2. ReLU\n",
        "### 3. Conv2d with kernel_size=1, stride=1, bias=False, all others default values\n",
        "### 4. AvgPool2d with kernel_size=2, stride=2, all others default values\n",
        "\n",
        "#### For a subclass of nn.Sequential, you only need to define `__init__` (the `forward` function is already defined in nn.Sequential)\n",
        "\n",
        "## Fill the blanks marked by \"##ToAdd:\" in the following cell (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZUJb7PFdLKm"
      },
      "source": [
        "class ConvPool(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            num_input_features: int, num_features for the BatchNorm2d layer\n",
        "            num_output_features: int, out_channels for the Conv2d layer\n",
        "        \"\"\"\n",
        "        super(ConvPool, self).__init__()\n",
        "        ##ToAdd: add the layers specified above; hint: use add_module function, e.g., self.add_module('norm', nn.BatchNorm2d(num_input_features)) (10 points)\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu',nn.ReLU(inplace=True))\n",
        "        self.add_module('conv2d1',nn.Conv2d(num_input_features,num_output_features, \n",
        "                                            kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('AvgPool2d1',nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        ##EndAdd"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kVduQWXdLKm"
      },
      "source": [
        "## DenseNet is more complex. It consists multiple Block layers (Note each Block layer consists of multiple CompositeLayer), specified by `block_config`. \n",
        "## The model architecture is as follows:\n",
        "### First convolutional layer (already defined for you) to process the input\n",
        "### A number of Block layers and ConvPool layers you need to add\n",
        "### Final layers such as AvgPool2d and Linear to predict the class labels (already defined for you)\n",
        "\n",
        "\n",
        "## Fill the blanks marked by \"##ToAdd:\" in the following cell (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibQBkrmfdLKn"
      },
      "source": [
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, block_config=(6, 12, 24, 16), num_init_features=64,\n",
        "                 num_hidden_features=128, num_output_features=32, num_classes=10):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            block_config: list of ints, how many layers in Block layer\n",
        "            num_init_features: int, num_input_features for the FIRST Block layer. (You need to calculate the num_input_features for all other Block layers)\n",
        "            num_hidden_features: int, num_hidden_features for ALL Block layers\n",
        "            num_output_features: int, num_output_features for ALL Block layers\n",
        "            num_classes: int, number of classes, out_features for the last Linear layer (already defined for you)\n",
        "        \"\"\"\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # First convolution already defined for you\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))\n",
        "\n",
        "        # A number of Block and ConvPool layers\n",
        "        num_features = num_init_features #num_features is used as the num_input_features for each Block layer\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            ##ToAdd: Define a Block layer with num_input_features=num_features (10 points)\n",
        "            block = Block(num_layers,\n",
        "                          num_input_features=num_features,\n",
        "                          num_hidden_features=num_hidden_features,\n",
        "                          num_output_features = num_output_features )\n",
        "            \n",
        "\n",
        "            ##EndAdd\n",
        "            \n",
        "    \n",
        "            ##ToAdd: calculate num_features as the output of the Block layer (5 points)\n",
        "            self.features.add_module('comp_layer%d' % (i+1), block)\n",
        "            num_features = num_features + num_layers * num_output_features \n",
        "            ## EndAdd\n",
        "            if i != len(block_config) - 1:\n",
        "                # All Block layers Except the last one is followed by a ConvPool layer\n",
        "                # Important: ConvPool layer will reduce the number of features by half\n",
        "                ##ToAdd: add a ConvPool layer with num_input_features=num_features, num_output_features=num_features//2 (5 points)\n",
        "                convpool = ConvPool(num_input_features=num_features, num_output_features=num_features//2)\n",
        "                ##EndAdd\n",
        "                self.features.add_module(f'convpool{i+1}', convpool)\n",
        "                # Here we update the num_features, which will be used for the next Block layer in this for loop\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module(f'norm{len(block_config)+1}', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO5o0ZYVdLKn"
      },
      "source": [
        "## Well done!\n",
        "## You don't need to change the following cells. They are here for you to learn how to train a computer vision model in practice\n",
        "## You can also test your implemented DenseNet model using it\n",
        "## If you can build a good DenseNet model (change `block_config`, `num_init_features`, `num_hidden_features`, `num_output_features`) and train it well (change optimizer, learning rate, num_epochs, etc.), you will get some extra credit in case you haven't gotten a perfect score. You are highly recommended to do so!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kReD-hPOdLKo",
        "outputId": "5dc2821e-ebe6-4d0d-f678-be3abac8fe23"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset, validationset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKEylL7WdLKp",
        "outputId": "676d18bf-ea80-4303-8d3c-274124eda3e8"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
        "                                          shuffle=True, pin_memory=True, num_workers=4)\n",
        "validationloader = torch.utils.data.DataLoader(validationset, batch_size=1024,\n",
        "                                          shuffle=False, pin_memory=True, num_workers=4)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1024,\n",
        "                                         shuffle=False, pin_memory=True, num_workers=4)\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5QGeEwqdLKp"
      },
      "source": [
        "writer = SummaryWriter('runs/cifar10')\n",
        "#model = DenseNet((6, 12, 24, 16), num_init_features=128, num_hidden_features=256, num_output_features=256, num_classes=10)\n",
        "#model = DenseNet((4,8), num_init_features=64, num_hidden_features=128, num_output_features=32, num_classes=10)\n",
        "model = DenseNet((4,8), num_init_features=128, num_hidden_features=256, num_output_features=256, num_classes=10)\n",
        "writer.add_graph(model, torch.randn(1, 3, 32, 32))\n",
        "writer.close()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaFCyHsOdLKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c4d0c0-70a4-40c0-a71d-ca59a15eda58"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = torch.nn.DataParallel(model).to(device)\n",
        "else:\n",
        "    model = model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1,\n",
        "                            momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "def get_acc(model, loader):\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        for i, data in enumerate(loader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            correct += (labels == outputs.argmax(dim=1).squeeze()).sum()\n",
        "        acc = float(correct) / len(loader.dataset)\n",
        "    return acc"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YwsFZo7dLKp",
        "outputId": "b6ee9c3d-9bdd-409d-f7ba-519c142b0bf2"
      },
      "source": [
        "num_epochs = 80\n",
        "eval_every = 100\n",
        "train_acc_his = []\n",
        "val_acc_his = []\n",
        "test_acc_his = []\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i==0 or (i+1) % eval_every == 0:\n",
        "            train_acc = get_acc(model, trainloader)\n",
        "            val_acc = get_acc(model, validationloader)\n",
        "            test_acc = get_acc(model, testloader)\n",
        "            train_acc_his.append(train_acc)\n",
        "            val_acc_his.append(val_acc)\n",
        "            test_acc_his.append(test_acc)\n",
        "            writer.add_scalar('train_acc', train_acc, len(train_acc_his))\n",
        "            writer.add_scalar('val_acc', val_acc, len(val_acc_his))\n",
        "            writer.add_scalar('test_acc', test_acc, len(test_acc_his))\n",
        "            print('[epoch %d, iter %d] train_acc: %.3f  val_acc: %.3f  test_acc: %.3f' %\n",
        "                  (epoch+1, i+1, train_acc, val_acc, test_acc))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 1, iter 1] train_acc: 0.135  val_acc: 0.140  test_acc: 0.134\n",
            "[epoch 1, iter 100] train_acc: 0.318  val_acc: 0.322  test_acc: 0.319\n",
            "[epoch 2, iter 1] train_acc: 0.365  val_acc: 0.361  test_acc: 0.369\n",
            "[epoch 2, iter 100] train_acc: 0.458  val_acc: 0.458  test_acc: 0.449\n",
            "[epoch 3, iter 1] train_acc: 0.459  val_acc: 0.449  test_acc: 0.456\n",
            "[epoch 3, iter 100] train_acc: 0.536  val_acc: 0.531  test_acc: 0.530\n",
            "[epoch 4, iter 1] train_acc: 0.595  val_acc: 0.587  test_acc: 0.580\n",
            "[epoch 4, iter 100] train_acc: 0.617  val_acc: 0.595  test_acc: 0.590\n",
            "[epoch 5, iter 1] train_acc: 0.634  val_acc: 0.611  test_acc: 0.611\n",
            "[epoch 5, iter 100] train_acc: 0.668  val_acc: 0.637  test_acc: 0.633\n",
            "[epoch 6, iter 1] train_acc: 0.689  val_acc: 0.648  test_acc: 0.650\n",
            "[epoch 6, iter 100] train_acc: 0.708  val_acc: 0.661  test_acc: 0.666\n",
            "[epoch 7, iter 1] train_acc: 0.731  val_acc: 0.674  test_acc: 0.679\n",
            "[epoch 7, iter 100] train_acc: 0.782  val_acc: 0.712  test_acc: 0.714\n",
            "[epoch 8, iter 1] train_acc: 0.787  val_acc: 0.709  test_acc: 0.711\n",
            "[epoch 8, iter 100] train_acc: 0.798  val_acc: 0.713  test_acc: 0.708\n",
            "[epoch 9, iter 1] train_acc: 0.804  val_acc: 0.717  test_acc: 0.711\n",
            "[epoch 9, iter 100] train_acc: 0.837  val_acc: 0.734  test_acc: 0.733\n",
            "[epoch 10, iter 1] train_acc: 0.850  val_acc: 0.740  test_acc: 0.730\n",
            "[epoch 10, iter 100] train_acc: 0.863  val_acc: 0.735  test_acc: 0.727\n",
            "[epoch 11, iter 1] train_acc: 0.829  val_acc: 0.712  test_acc: 0.708\n",
            "[epoch 11, iter 100] train_acc: 0.894  val_acc: 0.744  test_acc: 0.742\n",
            "[epoch 12, iter 1] train_acc: 0.891  val_acc: 0.739  test_acc: 0.735\n",
            "[epoch 12, iter 100] train_acc: 0.913  val_acc: 0.747  test_acc: 0.744\n",
            "[epoch 13, iter 1] train_acc: 0.896  val_acc: 0.743  test_acc: 0.739\n",
            "[epoch 13, iter 100] train_acc: 0.912  val_acc: 0.747  test_acc: 0.740\n",
            "[epoch 14, iter 1] train_acc: 0.936  val_acc: 0.752  test_acc: 0.748\n",
            "[epoch 14, iter 100] train_acc: 0.943  val_acc: 0.751  test_acc: 0.752\n",
            "[epoch 15, iter 1] train_acc: 0.938  val_acc: 0.761  test_acc: 0.752\n",
            "[epoch 15, iter 100] train_acc: 0.960  val_acc: 0.762  test_acc: 0.757\n",
            "[epoch 16, iter 1] train_acc: 0.933  val_acc: 0.743  test_acc: 0.744\n",
            "[epoch 16, iter 100] train_acc: 0.965  val_acc: 0.763  test_acc: 0.758\n",
            "[epoch 17, iter 1] train_acc: 0.961  val_acc: 0.761  test_acc: 0.757\n",
            "[epoch 17, iter 100] train_acc: 0.977  val_acc: 0.774  test_acc: 0.765\n",
            "[epoch 18, iter 1] train_acc: 0.966  val_acc: 0.757  test_acc: 0.754\n",
            "[epoch 18, iter 100] train_acc: 0.966  val_acc: 0.757  test_acc: 0.758\n",
            "[epoch 19, iter 1] train_acc: 0.971  val_acc: 0.764  test_acc: 0.761\n",
            "[epoch 19, iter 100] train_acc: 0.981  val_acc: 0.765  test_acc: 0.773\n",
            "[epoch 20, iter 1] train_acc: 0.970  val_acc: 0.766  test_acc: 0.758\n",
            "[epoch 20, iter 100] train_acc: 0.979  val_acc: 0.767  test_acc: 0.763\n",
            "[epoch 21, iter 1] train_acc: 0.977  val_acc: 0.765  test_acc: 0.762\n",
            "[epoch 21, iter 100] train_acc: 0.984  val_acc: 0.776  test_acc: 0.762\n",
            "[epoch 22, iter 1] train_acc: 0.980  val_acc: 0.768  test_acc: 0.764\n",
            "[epoch 22, iter 100] train_acc: 0.979  val_acc: 0.762  test_acc: 0.762\n",
            "[epoch 23, iter 1] train_acc: 0.989  val_acc: 0.775  test_acc: 0.772\n",
            "[epoch 23, iter 100] train_acc: 0.980  val_acc: 0.765  test_acc: 0.757\n",
            "[epoch 24, iter 1] train_acc: 0.982  val_acc: 0.764  test_acc: 0.765\n",
            "[epoch 24, iter 100] train_acc: 0.986  val_acc: 0.774  test_acc: 0.771\n",
            "[epoch 25, iter 1] train_acc: 0.986  val_acc: 0.767  test_acc: 0.759\n",
            "[epoch 25, iter 100] train_acc: 0.983  val_acc: 0.764  test_acc: 0.760\n",
            "[epoch 26, iter 1] train_acc: 0.975  val_acc: 0.759  test_acc: 0.750\n",
            "[epoch 26, iter 100] train_acc: 0.987  val_acc: 0.774  test_acc: 0.770\n",
            "[epoch 27, iter 1] train_acc: 0.984  val_acc: 0.771  test_acc: 0.760\n",
            "[epoch 27, iter 100] train_acc: 0.986  val_acc: 0.765  test_acc: 0.762\n",
            "[epoch 28, iter 1] train_acc: 0.980  val_acc: 0.764  test_acc: 0.761\n",
            "[epoch 28, iter 100] train_acc: 0.983  val_acc: 0.758  test_acc: 0.759\n",
            "[epoch 29, iter 1] train_acc: 0.984  val_acc: 0.765  test_acc: 0.764\n",
            "[epoch 29, iter 100] train_acc: 0.994  val_acc: 0.779  test_acc: 0.777\n",
            "[epoch 30, iter 1] train_acc: 0.995  val_acc: 0.781  test_acc: 0.778\n",
            "[epoch 30, iter 100] train_acc: 0.998  val_acc: 0.786  test_acc: 0.783\n",
            "[epoch 31, iter 1] train_acc: 0.998  val_acc: 0.782  test_acc: 0.780\n",
            "[epoch 31, iter 100] train_acc: 0.998  val_acc: 0.783  test_acc: 0.782\n",
            "[epoch 32, iter 1] train_acc: 0.998  val_acc: 0.790  test_acc: 0.784\n",
            "[epoch 32, iter 100] train_acc: 0.999  val_acc: 0.784  test_acc: 0.784\n",
            "[epoch 33, iter 1] train_acc: 0.998  val_acc: 0.782  test_acc: 0.777\n",
            "[epoch 33, iter 100] train_acc: 0.996  val_acc: 0.777  test_acc: 0.776\n",
            "[epoch 34, iter 1] train_acc: 0.996  val_acc: 0.777  test_acc: 0.778\n",
            "[epoch 34, iter 100] train_acc: 0.998  val_acc: 0.782  test_acc: 0.777\n",
            "[epoch 35, iter 1] train_acc: 0.993  val_acc: 0.775  test_acc: 0.767\n",
            "[epoch 35, iter 100] train_acc: 0.984  val_acc: 0.767  test_acc: 0.764\n",
            "[epoch 36, iter 1] train_acc: 0.972  val_acc: 0.759  test_acc: 0.751\n",
            "[epoch 36, iter 100] train_acc: 0.979  val_acc: 0.761  test_acc: 0.756\n",
            "[epoch 37, iter 1] train_acc: 0.978  val_acc: 0.760  test_acc: 0.754\n",
            "[epoch 37, iter 100] train_acc: 0.991  val_acc: 0.776  test_acc: 0.770\n",
            "[epoch 38, iter 1] train_acc: 0.994  val_acc: 0.780  test_acc: 0.777\n",
            "[epoch 38, iter 100] train_acc: 0.995  val_acc: 0.778  test_acc: 0.771\n",
            "[epoch 39, iter 1] train_acc: 0.987  val_acc: 0.759  test_acc: 0.764\n",
            "[epoch 39, iter 100] train_acc: 0.990  val_acc: 0.772  test_acc: 0.771\n",
            "[epoch 40, iter 1] train_acc: 0.992  val_acc: 0.772  test_acc: 0.770\n",
            "[epoch 40, iter 100] train_acc: 0.996  val_acc: 0.785  test_acc: 0.774\n",
            "[epoch 41, iter 1] train_acc: 0.991  val_acc: 0.773  test_acc: 0.768\n",
            "[epoch 41, iter 100] train_acc: 0.993  val_acc: 0.776  test_acc: 0.770\n",
            "[epoch 42, iter 1] train_acc: 0.991  val_acc: 0.772  test_acc: 0.771\n",
            "[epoch 42, iter 100] train_acc: 0.988  val_acc: 0.767  test_acc: 0.767\n",
            "[epoch 43, iter 1] train_acc: 0.982  val_acc: 0.756  test_acc: 0.756\n",
            "[epoch 43, iter 100] train_acc: 0.990  val_acc: 0.770  test_acc: 0.766\n",
            "[epoch 44, iter 1] train_acc: 0.988  val_acc: 0.765  test_acc: 0.762\n",
            "[epoch 44, iter 100] train_acc: 0.987  val_acc: 0.768  test_acc: 0.763\n",
            "[epoch 45, iter 1] train_acc: 0.988  val_acc: 0.772  test_acc: 0.768\n",
            "[epoch 45, iter 100] train_acc: 0.989  val_acc: 0.764  test_acc: 0.772\n",
            "[epoch 46, iter 1] train_acc: 0.987  val_acc: 0.771  test_acc: 0.765\n",
            "[epoch 46, iter 100] train_acc: 0.990  val_acc: 0.770  test_acc: 0.771\n",
            "[epoch 47, iter 1] train_acc: 0.992  val_acc: 0.768  test_acc: 0.765\n",
            "[epoch 47, iter 100] train_acc: 0.991  val_acc: 0.776  test_acc: 0.773\n",
            "[epoch 48, iter 1] train_acc: 0.993  val_acc: 0.768  test_acc: 0.771\n",
            "[epoch 48, iter 100] train_acc: 0.997  val_acc: 0.784  test_acc: 0.779\n",
            "[epoch 49, iter 1] train_acc: 0.996  val_acc: 0.776  test_acc: 0.776\n",
            "[epoch 49, iter 100] train_acc: 0.996  val_acc: 0.777  test_acc: 0.774\n",
            "[epoch 50, iter 1] train_acc: 0.997  val_acc: 0.779  test_acc: 0.785\n",
            "[epoch 50, iter 100] train_acc: 0.998  val_acc: 0.784  test_acc: 0.783\n",
            "[epoch 51, iter 1] train_acc: 0.998  val_acc: 0.784  test_acc: 0.787\n",
            "[epoch 51, iter 100] train_acc: 0.999  val_acc: 0.787  test_acc: 0.784\n",
            "[epoch 52, iter 1] train_acc: 0.998  val_acc: 0.784  test_acc: 0.783\n",
            "[epoch 52, iter 100] train_acc: 0.999  val_acc: 0.782  test_acc: 0.779\n",
            "[epoch 53, iter 1] train_acc: 0.999  val_acc: 0.783  test_acc: 0.781\n",
            "[epoch 53, iter 100] train_acc: 0.999  val_acc: 0.782  test_acc: 0.782\n",
            "[epoch 54, iter 1] train_acc: 0.999  val_acc: 0.787  test_acc: 0.785\n",
            "[epoch 54, iter 100] train_acc: 0.996  val_acc: 0.778  test_acc: 0.776\n",
            "[epoch 55, iter 1] train_acc: 0.981  val_acc: 0.756  test_acc: 0.755\n",
            "[epoch 55, iter 100] train_acc: 0.983  val_acc: 0.759  test_acc: 0.762\n",
            "[epoch 56, iter 1] train_acc: 0.977  val_acc: 0.762  test_acc: 0.756\n",
            "[epoch 56, iter 100] train_acc: 0.982  val_acc: 0.763  test_acc: 0.759\n",
            "[epoch 57, iter 1] train_acc: 0.982  val_acc: 0.765  test_acc: 0.762\n",
            "[epoch 57, iter 100] train_acc: 0.990  val_acc: 0.773  test_acc: 0.774\n",
            "[epoch 58, iter 1] train_acc: 0.992  val_acc: 0.774  test_acc: 0.773\n",
            "[epoch 58, iter 100] train_acc: 0.992  val_acc: 0.778  test_acc: 0.772\n",
            "[epoch 59, iter 1] train_acc: 0.991  val_acc: 0.766  test_acc: 0.766\n",
            "[epoch 59, iter 100] train_acc: 0.987  val_acc: 0.763  test_acc: 0.765\n",
            "[epoch 60, iter 1] train_acc: 0.986  val_acc: 0.771  test_acc: 0.765\n",
            "[epoch 60, iter 100] train_acc: 0.993  val_acc: 0.780  test_acc: 0.769\n",
            "[epoch 61, iter 1] train_acc: 0.994  val_acc: 0.774  test_acc: 0.772\n",
            "[epoch 61, iter 100] train_acc: 0.997  val_acc: 0.778  test_acc: 0.780\n",
            "[epoch 62, iter 1] train_acc: 0.992  val_acc: 0.769  test_acc: 0.775\n",
            "[epoch 62, iter 100] train_acc: 0.992  val_acc: 0.776  test_acc: 0.774\n",
            "[epoch 63, iter 1] train_acc: 0.991  val_acc: 0.769  test_acc: 0.768\n",
            "[epoch 63, iter 100] train_acc: 0.993  val_acc: 0.765  test_acc: 0.774\n",
            "[epoch 64, iter 1] train_acc: 0.992  val_acc: 0.768  test_acc: 0.771\n",
            "[epoch 64, iter 100] train_acc: 0.989  val_acc: 0.769  test_acc: 0.765\n",
            "[epoch 65, iter 1] train_acc: 0.989  val_acc: 0.770  test_acc: 0.767\n",
            "[epoch 65, iter 100] train_acc: 0.990  val_acc: 0.779  test_acc: 0.773\n",
            "[epoch 66, iter 1] train_acc: 0.986  val_acc: 0.762  test_acc: 0.765\n",
            "[epoch 66, iter 100] train_acc: 0.985  val_acc: 0.766  test_acc: 0.765\n",
            "[epoch 67, iter 1] train_acc: 0.986  val_acc: 0.763  test_acc: 0.769\n",
            "[epoch 67, iter 100] train_acc: 0.993  val_acc: 0.780  test_acc: 0.779\n",
            "[epoch 68, iter 1] train_acc: 0.995  val_acc: 0.782  test_acc: 0.786\n",
            "[epoch 68, iter 100] train_acc: 0.996  val_acc: 0.782  test_acc: 0.781\n",
            "[epoch 69, iter 1] train_acc: 0.996  val_acc: 0.778  test_acc: 0.778\n",
            "[epoch 69, iter 100] train_acc: 0.994  val_acc: 0.772  test_acc: 0.781\n",
            "[epoch 70, iter 1] train_acc: 0.993  val_acc: 0.776  test_acc: 0.770\n",
            "[epoch 70, iter 100] train_acc: 0.994  val_acc: 0.775  test_acc: 0.772\n",
            "[epoch 71, iter 1] train_acc: 0.993  val_acc: 0.775  test_acc: 0.768\n",
            "[epoch 71, iter 100] train_acc: 0.995  val_acc: 0.777  test_acc: 0.774\n",
            "[epoch 72, iter 1] train_acc: 0.996  val_acc: 0.775  test_acc: 0.777\n",
            "[epoch 72, iter 100] train_acc: 0.998  val_acc: 0.787  test_acc: 0.779\n",
            "[epoch 73, iter 1] train_acc: 0.995  val_acc: 0.779  test_acc: 0.770\n",
            "[epoch 73, iter 100] train_acc: 0.992  val_acc: 0.771  test_acc: 0.768\n",
            "[epoch 74, iter 1] train_acc: 0.991  val_acc: 0.767  test_acc: 0.770\n",
            "[epoch 74, iter 100] train_acc: 0.976  val_acc: 0.750  test_acc: 0.745\n",
            "[epoch 75, iter 1] train_acc: 0.980  val_acc: 0.760  test_acc: 0.763\n",
            "[epoch 75, iter 100] train_acc: 0.989  val_acc: 0.775  test_acc: 0.768\n",
            "[epoch 76, iter 1] train_acc: 0.988  val_acc: 0.770  test_acc: 0.762\n",
            "[epoch 76, iter 100] train_acc: 0.991  val_acc: 0.767  test_acc: 0.771\n",
            "[epoch 77, iter 1] train_acc: 0.995  val_acc: 0.778  test_acc: 0.779\n",
            "[epoch 77, iter 100] train_acc: 0.995  val_acc: 0.774  test_acc: 0.774\n",
            "[epoch 78, iter 1] train_acc: 0.993  val_acc: 0.776  test_acc: 0.774\n",
            "[epoch 78, iter 100] train_acc: 0.997  val_acc: 0.778  test_acc: 0.773\n",
            "[epoch 79, iter 1] train_acc: 0.994  val_acc: 0.775  test_acc: 0.772\n",
            "[epoch 79, iter 100] train_acc: 0.997  val_acc: 0.779  test_acc: 0.777\n",
            "[epoch 80, iter 1] train_acc: 0.993  val_acc: 0.774  test_acc: 0.776\n",
            "[epoch 80, iter 100] train_acc: 0.995  val_acc: 0.779  test_acc: 0.774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "bmegmtaidLKq",
        "outputId": "143ac38d-f530-47d4-fa48-441efb645a5a"
      },
      "source": [
        "plt.plot(train_acc_his, 'ro-', label='train')\n",
        "plt.plot(val_acc_his, 'bv-', label='validation')\n",
        "plt.plot(test_acc_his, 'g+-', label='test')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Step')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}